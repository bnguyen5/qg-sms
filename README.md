# QG-SMS: Enhancing Test Item Analysis via Student Modeling and Simulation
Implementation details for our ACL'25 paper: [QG-SMS: Enhancing Test Item Analysis via Student Modeling and Simulation](https://arxiv.org/abs/2503.05888)


**Abstract**: While the Question Generation (QG) task has been increasingly adopted in educational assessments, its evaluation remains limited by approaches that lack a clear connection to the educational values of test items. In this work, we introduce test item analysis, a method frequently used by educators to assess test question quality, into QG evaluation. Specifically, we construct pairs of candidate questions that differ in quality across dimensions such as topic coverage, item difficulty, item discrimination, and distractor efficiency. We then examine whether existing QG evaluation approaches can effectively distinguish these differences. Our findings reveal significant shortcomings in these approaches with respect to accurately assessing test item quality in relation to student performance. To address this gap, we propose a novel QG evaluation framework, QG-SMS, which leverages Large Language Model for Student Modeling and Simulation to perform test item analysis. As demonstrated in our extensive experiments and human evaluation study, the additional perspectives introduced by the simulated student profiles lead to a more effective and robust assessment of test items.


---

## Install dependencies:

This code has been tested with Python 3.9.21

```bash
pip install -r requirements.txt
```
    
---

## Data Preprocessing (for evaluating QG-SMS)

This step prepares your raw question data. It calculates key psychometric dimensions, including **difficulty**, **discrimination**, and **distractor efficiency**, then assembles question pairs that differ across these dimensions to test the performance of QG-SMS.

**Input File Format:**

Your input JSON file should be a **list of dictionaries**. Each dictionary represents a single question and **must** include the following top-level keys:

* `q_id` (string/integer): A unique identifier for the question.
* `learning_mat_id` (string/integer): An identifier for the associated learning material.
* `q_text` (string): The full text of the question.
* `answer` (string): The correct answer choice for the question.
* `learning_mat` (string): The complete text of the learning material related to this question.
* `topic` (string): The subject or topic of the question.
* `stud_perf` (list of dicts): A list detailing individual student performances on this specific question. Each dictionary within `stud_perf` **must** have:
    * `accuracy` (string): Either `"correct"` or `"incorrect"`.
    * `s_id` (string/integer): A unique identifier for the student.
    * `choice` (string): The answer choice the student selected.

**Example of a valid question dictionary:**

```json
[
  {
    "q_id": "Q_Geo_001",
    "learning_mat_id": "LM_EU_Capitals",
    "q_text": "Which city is the capital of France?",
    "answer": "Paris",
    "learning_mat": "France, a country in Western Europe, is famous for its capital city, Paris, situated on the Seine River.",
    "topic": "Geography",
    "stud_perf": [
      {
        "accuracy": "correct",
        "s_id": "STU001",
        "choice": "Paris"
      },
      {
        "accuracy": "incorrect",
        "s_id": "STU002",
        "choice": "London"
      },
      {
        "accuracy": "correct",
        "s_id": "STU003",
        "choice": "Paris"
      }
    ]
  }
]
```

**How to Run**

```bash
cd datasets
python3 preprocess_data.py --input_file /path/to/your/raw_questions.json --output_dir /path/to/your/output_folder
```


## Run QG-SMS
You can evaluate QG-SMS using the `pair_dataset.json` file generated by the data preprocessing step or you can use QG-SMS to evaluate question quality with your own data.

**Input File Format:**

The input JSON file (which should be named `pair_dataset.json` and placed within the directory specified by the `--path` argument) must be a list of dictionaries. Each dictionary represents a question pair and must contain these keys:

* `test_id` (string/integer): A unique identifier for the question pair.
* `l_id` (string/integer): The ID of the common learning material for the pair.
* `learning_mat` (string): The full text of the common learning material for the pair.
* `question_1` (string): The text of the first question in the pair.
* `question_2` (string): The text of the second question in the pair.
* `requirement` (string): A description of the desired quality being evaluated (e.g., "the question that is easier to answer")
* `requirement_description` (string): A detailed explanation of the requirement.
* `label` (integer: 1 or 2): Only required if you include the `--eval` flag when running the script to evaluate QG-SMS. This integer (1 or 2) indicates which question (question_1 or question_2) in the pair best meets the specified requirement according to your ground truth.

**How to Run**

You'll need an OpenAI API key to run this step. 

```bash
cd qg-sms
python3 evaluate.py --path ../path/to/your/pair_dataset/ --api_key [YOUR_OPENAI_KEY] --eval
```

**Output Files**: The script will save its results directly within the directory you specify with --path:

- `gen_students_by_L.json`: Contains the simulated student profiles generated by the LLM for each unique learning material.
- `eval_result.json`: Provides detailed results for each question pair, including the LLM's full completion for the evaluation task and its determined "winner" (which question it chose).
- `statistics.json`: (Only generated if the --eval flag is used) This file contains aggregate evaluation metrics, such as average accuracy, consistent accuracy and Cohen's Kappa score, comparing the LLM's choices against your provided ground truth labels.
